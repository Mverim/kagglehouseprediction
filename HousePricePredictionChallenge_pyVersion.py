# -*- coding: utf-8 -*-
"""HousePricePredictionChallenge.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1312p9u8-XAWRFmnUqzRHBt76xnrsPrEZ

#Muhammed Verim
#110160023
"""

#!pip install pycaret
#!pip install optuna

"""Part above was used for compering models and finding parameters by using pycaret and optuna respectively. Commented out to gain compiling speed."""

import numpy as np
import pandas as pd
pd.pandas.set_option('display.max_columns', None)
pd.pandas.set_option('display.max_rows', 100)

import matplotlib.pyplot as plt
import seaborn as sns
sns.set_style('darkgrid')

from sklearn.neighbors import KNeighborsRegressor
import scipy.stats
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import KFold, cross_val_score

from sklearn.model_selection import GridSearchCV

from lightgbm import LGBMRegressor
from sklearn.ensemble import ExtraTreesRegressor,GradientBoostingRegressor
from sklearn.linear_model import BayesianRidge,ElasticNet

#from pycaret.regression import setup, compare_models
#import optuna

traindata=pd.read_csv('train.csv')
testdata=pd.read_csv('test.csv')

"""#Test train data combine"""

target = traindata['SalePrice']
train_id = traindata['Id']
test_id = testdata['Id']

traindata1 = traindata.drop(['Id', 'SalePrice'], axis=1)
testdata1 = testdata.drop('Id', axis=1)

data1 = pd.concat([traindata1, testdata1], axis=0).reset_index(drop=True)

"""#Cleaning"""

data2 = data1.copy()

"""##Ensure Data Type
Looking at the data description, only column that has numerical values that corresponds to is MSSubCLass so transforming it into a string type. Doing this to ensure the feature is categorical instead of numerical.
"""

data2['MSSubClass'] = data2['MSSubClass'].astype(str)

"""This many missing object values found."""

data2.select_dtypes(object).isna().sum()

"""This many missing numeric values found."""

data2.select_dtypes(np.number).isna().sum()

"""##Fill Categoric Missing Values

Now gonna transform these missing values properly.

For the columns with NA as set to a value in data description file, going to impute a constant 'None' value
"""

# Impute using the constant mode
for column in [
    'Alley',
    'BsmtQual',
    'BsmtCond',
    'BsmtExposure',
    'BsmtFinType1',
    'BsmtFinType2',
    'FireplaceQu',
    'GarageType',
    'GarageFinish',
    'GarageQual',
    'GarageCond',
    'PoolQC',
    'Fence',
    'MiscFeature'
]:
    data2[column] = data2[column].fillna("None")

"""For the columns with no NA description in description file, going to impute a mode 0."""

# Impute using the column mode
for column in [
    'MSZoning',
    'Utilities',
    'Exterior1st',
    'Exterior2nd',
    'MasVnrType',
    'Electrical',
    'KitchenQual',
    'Functional',
    'SaleType'
]:
    data2[column] = data2[column].fillna(data2[column].mode()[0])

data3 = data2.copy()

"""##Fill Numeric Missing Values

These are the numerical columns and nan values.
"""

nmdata3 = data3.select_dtypes(np.number)
nmdata3.isna().sum()

"""Since 'LotFrontage' has the most it is applicable for selecting and trying to locate nan values."""

nmdata3.loc[nmdata3['LotFrontage'].isna() == True, 'LotFrontage']

"""The columns with 0 nan values 


"""

non_na_columns = nmdata3.loc[ :, nmdata3.isna().sum() == 0].columns
nmdata3.loc[nmdata3['LotFrontage'].isna() == False, non_na_columns]

data3.loc[data3['LotFrontage'].isna() == True, 'LotFrontage']

"""#Feature Engineering

Some features were dropped after combining them together in this part.
"""

data3['TotalBathrooms'] = data3['BsmtFullBath'] + data3['FullBath'] + 0.5*(data3['BsmtHalfBath'] + data3['HalfBath'])

data3['TotalInsideSF'] = data3['BsmtFinSF1']+data3['BsmtFinSF2']+data3['BsmtUnfSF']+data3['TotalBsmtSF']+data3['1stFlrSF']+data3['2ndFlrSF']+data3['LowQualFinSF']+data3['GrLivArea']

data3['TotalOutsideSF'] = data3['WoodDeckSF']+data3['OpenPorchSF']+data3['EnclosedPorch']+data3['3SsnPorch']+data3['ScreenPorch']+data3['PoolArea']

data3 = data3.drop([
    'BsmtFullBath',
    'FullBath',
    'BsmtHalfBath',
    'HalfBath',
    'BsmtFinSF1',
    'BsmtFinSF2',
    'BsmtUnfSF',
    'TotalBsmtSF',
    '1stFlrSF',
    '2ndFlrSF',
    'LowQualFinSF',
    'GrLivArea',
    'WoodDeckSF',
    'OpenPorchSF',
    'EnclosedPorch',
    '3SsnPorch',
    'ScreenPorch',
    'PoolArea',
    ], axis=1)

quality_columns_1 =[
                'ExterQual', 
                'ExterCond', 
                'PoolQC',
                'HeatingQC', 
                'KitchenQual']
quality_columns_2 = [                
                'BsmtQual', 
                'BsmtCond',                 
                'FireplaceQu', 
                'GarageQual', 
                'GarageCond' ]
quality_columns_3 = ['BsmtFinType1','BsmtFinType2']

mapping1 = {'Ex': 1, 'Gd': 0.8, 'TA': 0.6, 'Fa': 0.3, 'Po': 0.1}
mapping2 = {'Ex': 1, 'Gd': 0.8, 'TA': 0.6, 'Fa': 0.4, 'Po': 0.2, 'NA': 0.1}
mapping3 = {'GLQ': 1, 'ALQ': 0.85, 'BLQ': 0.66, 'Rec': 0.5, 'LwQ': 0.25, 'Unf': 0.1, 'NA':0.01}

def Mapper(df,quality_column,map):
  for i in enumerate(quality_column):
    df[i[1]] = df[i[1]].map(map)

Mapper(data3,quality_columns_1,mapping1)
Mapper(data3,quality_columns_2,mapping2)
Mapper(data3,quality_columns_3,mapping3)

nmdata3 = data3.select_dtypes(np.number)
non_na_columns = nmdata3.loc[ :, nmdata3.isna().sum() == 0].columns

nmdata3.isna().sum()

"""Going to use knn regressor, have to impute these nan values with..."""

def knn_impute(df,na_target):
    df = df.copy()
    numericdf = df.select_dtypes(np.number)

    y_train = numericdf.loc[numericdf[na_target].isna() == False, na_target]
    X_train = numericdf.loc[numericdf[na_target].isna() == False, non_na_columns]
    X_test = numericdf.loc[numericdf[na_target].isna() == True, non_na_columns]
    knn = KNeighborsRegressor()
    knn.fit(X_train, y_train)
    
    y_pred = knn.predict(X_test)
    
    df.loc[df[na_target].isna() == True, na_target] = y_pred
    
    return df

for column in [
    'LotFrontage',
    'BsmtQual',
    'BsmtCond',
    'FireplaceQu',
    'GarageQual',
    'GarageCond',
    'BsmtFinType1',
    'BsmtFinType2',
    'PoolQC',
    'MasVnrArea',
    'GarageYrBlt',
    'GarageCars',
    'GarageArea'
]:
    data3 = knn_impute(data3, column)

data3.select_dtypes(np.number).columns

"""#Feature Transformation"""

data3.select_dtypes(np.number).columns

data4 = data3.copy()

data4.reset_index(drop=True)

"""These missing data points will be handled later by hard coding"""

data4[data4.isna().any(axis=1)]

"""This part was used to plot all numerical values, commented out for compile speed"""

# columns = ['LotFrontage', 'LotArea', 'OverallQual', 'OverallCond', 'YearBuilt',
          #  'YearRemodAdd', 'MasVnrArea', 'BedroomAbvGr', 'KitchenAbvGr', 
          #  'TotRmsAbvGrd', 'Fireplaces', 'GarageYrBlt', 'GarageCars', 
          #  'GarageArea', 'MiscVal', 'MoSold', 'YrSold', 
          #  'TotalBathrooms', 'TotalInsideSF', 'TotalOutsideSF']
# plt.figure(figsize = (15,30))
# for i in enumerate(columns):
#   plt.subplot(5, 4, i[0]+1)
#   sns.histplot(data4[i[1]], kde=True)

"""#Encoding categorical Values"""

data5 = pd.get_dummies(data4)

"""#Scaling"""

scaler = StandardScaler()
scaler.fit(data5)
data5 = pd.DataFrame(scaler.transform(data5), index=data5.index, columns=data5.columns)

"""#Train test data split
Seperating the train and test datasets after processing.
"""

train_final = data5.loc[:traindata.index.max(), :].copy()
test_final = data5.loc[traindata.index.max() + 1:, :].reset_index(drop=True).copy()

train_final.isna().sum().sum()

test_final.isna().sum().sum()

"""#Model Selection

This was the pycaret framework's part where 4 models were selected, commented out to minimize compile speed.
"""

#_ = setup(data=pd.concat([train_final, target], axis=1), target='SalePrice')

#compare_models()

"""#Target transformation

Target was transformed to log form in order to get a better prediction, it is returned back to normal later for submission.
"""

target

log_target = np.log(target)
log_target

"""#Model and appropriate parameter selection
These models were chosen to evaulate and seelct hyperparameters.

https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html

https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesRegressor.html

https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html

https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.BayesianRidge.html

#Bagging models
"""

models = {
    'lightgbm': LGBMRegressor(),
    'extratrees': ExtraTreesRegressor(),
    'gradientboost': GradientBoostingRegressor(random_state=0),
    'br': BayesianRidge()
}

"""#Parameters"""

lightgbm_params = {
    'num_leaves': 23,
    'max_depth': 2,
    'learning_rate': 0.11530900277518469,
    'n_estimators': 421
}
extratrees_params = {
    'n_estimators':505, 
    'criterion': 'squared_error', 
    'max_depth': 4, 
    'max_leaf_nodes':8
}
gradientboosting_params = {
    'max_depth': 2,
    'n_estimators': 317,
    'learning_rate': 0.21899751297362915,
    'max_features': 'sqrt',
    'alpha': 0.9
}
bayesianridge_params = {
    'n_iter':392,
    'tol':2.059480379517456e-05,
    'alpha_1':0.03185446208048185,
    'alpha_2':9.961608678132036,
    'lambda_1':5.563266834681936e-06,
    'lambda_2':5.513286028205375e-08
}

"""#Optuna parameter selection
This part is commented out, 4 models were tested individually and parameters found was selected as the values in the section above. 
"""

# def lgbm_objective(trial):
#     n_estimators = trial.suggest_int('n_estimators', 150, 600)
#     max_depth = trial.suggest_int('max_depth', 2, 5)
#     num_leaves = trial.suggest_int('num_leaves', 10, 25)
#     learning_rate = trial.suggest_loguniform('learning_rate', 0.11, 0.18)
    
#     model = LGBMRegressor(
#         n_estimators=n_estimators,
#         max_depth=max_depth,
#         num_leaves=num_leaves,
#         learning_rate=learning_rate
#     )
    
#     model.fit(train_final, log_target)
    
#     cv_scores = np.exp(np.sqrt(-cross_val_score(model, train_final, log_target, scoring='neg_mean_squared_error', cv=kf)))
    
#     return np.mean(cv_scores)

# study = optuna.create_study(direction='minimize')
# study.optimize(lgbm_objective, n_trials=100)
# study.best_params

"""#Training the models"""

kf = KFold(n_splits=10)

models = {
    "extratrees": ExtraTreesRegressor(**extratrees_params, verbose=0),
    "br": BayesianRidge(**bayesianridge_params),
    "lightgbm": LGBMRegressor(**lightgbm_params),
    "gradientboost": GradientBoostingRegressor(**gradientboosting_params)
}

for name, model in models.items():
    model.fit(train_final, log_target)
    print(name + " trained.")

results = {}

for name, model in models.items():
    result = np.exp(np.sqrt(-cross_val_score(model, train_final, log_target, scoring='neg_mean_squared_error', cv=kf)))
    results[name] = result

for name, result in results.items():
    print("----------\n" + name)
    print(np.mean(result))
    print(np.std(result))

"""#Final Prediction

This part had to be hard coded since there were 3 distinct missing data points.
"""

test_final[test_final.isna().any(axis=1)]
test_final['TotalBathrooms'][660] = 0
test_final['TotalBathrooms'][728] = 0
test_final['TotalInsideSF'][660] = 0

"""All the weight fraction changes were made in this part by hand by hard coding"""

final_predictions = (
    0.05 * np.exp(models['extratrees'].predict(test_final)) +
    0.10 * np.exp(models['br'].predict(test_final)) +
    0.75 * np.exp(models['lightgbm'].predict(test_final)) +
    0.10 * np.exp(models['gradientboost'].predict(test_final))
)

final_predictions

"""#Submission"""

submission = pd.concat([test_id, pd.Series(final_predictions, name='SalePrice')], axis=1)
submission

submission.to_csv('./submission.csv', index=False, header=True)